{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMUrr5RDKQ2t/kgRAje7pL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RRRRRRL/hw3/blob/main/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "D7YMJDiUcHHl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetGenerator(nn.Module):\n",
        "    \"\"\"Create a Unet-based generator\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        \"\"\"Construct a Unet generator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            output_nc (int) -- the number of channels in output images\n",
        "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
        "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
        "            ngf (int)       -- the number of filters in the last conv layer\n",
        "            norm_layer      -- normalization layer\n",
        "\n",
        "        We construct the U-Net from the innermost layer to the outermost layer.\n",
        "        It is a recursive process.\n",
        "        \"\"\"\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        # construct unet structure\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
        "        for i in range(num_downs - 5):  # add intermediate layers with ngf * 8 filters\n",
        "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward\"\"\"\n",
        "        return self.model(input)"
      ],
      "metadata": {
        "id": "bcvrrPF7bHRC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLayerDiscriminator(nn.Module):\n",
        "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a PatchGAN discriminator\n",
        "\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        kw = 4\n",
        "        padw = 1\n",
        "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2**n, 8)\n",
        "            sequence += [nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True)]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2**n_layers, 8)\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias), norm_layer(ndf * nf_mult), nn.LeakyReLU(0.2, True)]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(input)"
      ],
      "metadata": {
        "id": "nakHeuVRbXq2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class Pix2PixModel(BaseModel):\n",
        "    \"\"\"This class implements the pix2pix model, for learning a mapping from input images to output images given paired data.\n",
        "\n",
        "    The model training requires '--dataset_mode aligned' dataset.\n",
        "    By default, it uses a '--netG unet256' U-Net generator,\n",
        "    a '--netD basic' discriminator (PatchGAN),\n",
        "    and a '--gan_mode' vanilla GAN loss (the cross-entropy objective used in the orignal GAN paper).\n",
        "\n",
        "    pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train=True):\n",
        "        \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n",
        "\n",
        "        Parameters:\n",
        "            parser          -- original option parser\n",
        "            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n",
        "\n",
        "        Returns:\n",
        "            the modified parser.\n",
        "\n",
        "        For pix2pix, we do not use image buffer\n",
        "        The training objective is: GAN Loss + lambda_L1 * ||G(A)-B||_1\n",
        "        By default, we use vanilla GAN loss, UNet with batchnorm, and aligned datasets.\n",
        "        \"\"\"\n",
        "        # changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)\n",
        "        parser.set_defaults(norm=\"batch\", netG=\"unet_256\", dataset_mode=\"aligned\")\n",
        "        if is_train:\n",
        "            parser.set_defaults(pool_size=0, gan_mode=\"vanilla\")\n",
        "            parser.add_argument(\"--lambda_L1\", type=float, default=100.0, help=\"weight for L1 loss\")\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"Initialize the pix2pix class.\n",
        "\n",
        "        Parameters:\n",
        "            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
        "        \"\"\"\n",
        "        BaseModel.__init__(self, opt)\n",
        "        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n",
        "        self.loss_names = [\"G_GAN\", \"G_L1\", \"D_real\", \"D_fake\"]\n",
        "        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n",
        "        self.visual_names = [\"real_A\", \"fake_B\", \"real_B\"]\n",
        "        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n",
        "        if self.isTrain:\n",
        "            self.model_names = [\"G\", \"D\"]\n",
        "        else:  # during test time, only load G\n",
        "            self.model_names = [\"G\"]\n",
        "        self.device = opt.device\n",
        "        # define networks (both generator and discriminator)\n",
        "        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain)\n",
        "\n",
        "        if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n",
        "            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain)\n",
        "\n",
        "        if self.isTrain:\n",
        "            # define loss functions\n",
        "            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)  # move to the device for custom loss\n",
        "            self.criterionL1 = torch.nn.L1Loss()\n",
        "            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
        "            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "            self.optimizers.append(self.optimizer_G)\n",
        "            self.optimizers.append(self.optimizer_D)\n",
        "\n",
        "    def set_input(self, input):\n",
        "        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n",
        "\n",
        "        Parameters:\n",
        "            input (dict): include the data itself and its metadata information.\n",
        "\n",
        "        The option 'direction' can be used to swap images in domain A and domain B.\n",
        "        \"\"\"\n",
        "        AtoB = self.opt.direction == \"AtoB\"\n",
        "        self.real_A = input[\"A\" if AtoB else \"B\"].to(self.device)\n",
        "        self.real_B = input[\"B\" if AtoB else \"A\"].to(self.device)\n",
        "        self.image_paths = input[\"A_paths\" if AtoB else \"B_paths\"]\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
        "        self.fake_B = self.netG(self.real_A)  # G(A)\n",
        "\n",
        "    def backward_D(self):\n",
        "        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
        "        # Fake; stop backprop to the generator by detaching fake_B\n",
        "        fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n",
        "        pred_fake = self.netD(fake_AB.detach())\n",
        "        self.loss_D_fake = self.criterionGAN(pred_fake, False)\n",
        "        # Real\n",
        "        real_AB = torch.cat((self.real_A, self.real_B), 1)\n",
        "        pred_real = self.netD(real_AB)\n",
        "        self.loss_D_real = self.criterionGAN(pred_real, True)\n",
        "        # combine loss and calculate gradients\n",
        "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
        "        self.loss_D.backward()\n",
        "\n",
        "    def backward_G(self):\n",
        "        \"\"\"Calculate GAN and L1 loss for the generator\"\"\"\n",
        "        # First, G(A) should fake the discriminator\n",
        "        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n",
        "        pred_fake = self.netD(fake_AB)\n",
        "        self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n",
        "        # Second, G(A) = B\n",
        "        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n",
        "        # combine loss and calculate gradients\n",
        "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
        "        self.loss_G.backward()\n",
        "\n",
        "    def optimize_parameters(self):\n",
        "        self.forward()  # compute fake images: G(A)\n",
        "        # update D\n",
        "        self.set_requires_grad(self.netD, True)  # enable backprop for D\n",
        "        self.optimizer_D.zero_grad()  # set D's gradients to zero\n",
        "        self.backward_D()  # calculate gradients for D\n",
        "        self.optimizer_D.step()  # update D's weights\n",
        "        # update G\n",
        "        self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\n",
        "        self.optimizer_G.zero_grad()  # set G's gradients to zero\n",
        "        self.backward_G()  # calculate graidents for G\n",
        "        self.optimizer_G.step()  # update G's weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "GFxzCO6vbYnA",
        "outputId": "a174147e-c1ac-4733-a3fa-70357595b2a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "attempted relative import with no known parent package",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-615542628.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from data.base_dataset import BaseDataset, get_params, get_transform\n",
        "from data.image_folder import make_dataset\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class AlignedDataset(BaseDataset):\n",
        "    \"\"\"A dataset class for paired image dataset.\n",
        "\n",
        "    It assumes that the directory '/path/to/data/train' contains image pairs in the form of {A,B}.\n",
        "    During test time, you need to prepare a directory '/path/to/data/test'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"Initialize this dataset class.\n",
        "\n",
        "        Parameters:\n",
        "            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
        "        \"\"\"\n",
        "        BaseDataset.__init__(self, opt)\n",
        "        self.dir_AB = os.path.join(opt.dataroot, opt.phase)  # get the image directory\n",
        "        self.AB_paths = sorted(make_dataset(self.dir_AB, opt.max_dataset_size))  # get image paths\n",
        "        assert self.opt.load_size >= self.opt.crop_size  # crop_size should be smaller than the size of loaded image\n",
        "        self.input_nc = self.opt.output_nc if self.opt.direction == \"BtoA\" else self.opt.input_nc\n",
        "        self.output_nc = self.opt.input_nc if self.opt.direction == \"BtoA\" else self.opt.output_nc\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return a data point and its metadata information.\n",
        "\n",
        "        Parameters:\n",
        "            index - - a random integer for data indexing\n",
        "\n",
        "        Returns a dictionary that contains A, B, A_paths and B_paths\n",
        "            A (tensor) - - an image in the input domain\n",
        "            B (tensor) - - its corresponding image in the target domain\n",
        "            A_paths (str) - - image paths\n",
        "            B_paths (str) - - image paths (same as A_paths)\n",
        "        \"\"\"\n",
        "        # read a image given a random integer index\n",
        "        AB_path = self.AB_paths[index]\n",
        "        AB = Image.open(AB_path).convert(\"RGB\")\n",
        "        # split AB image into A and B\n",
        "        w, h = AB.size\n",
        "        w2 = int(w / 2)\n",
        "        A = AB.crop((0, 0, w2, h))\n",
        "        B = AB.crop((w2, 0, w, h))\n",
        "\n",
        "        # apply the same transform to both A and B\n",
        "        transform_params = get_params(self.opt, A.size)\n",
        "        A_transform = get_transform(self.opt, transform_params, grayscale=(self.input_nc == 1))\n",
        "        B_transform = get_transform(self.opt, transform_params, grayscale=(self.output_nc == 1))\n",
        "\n",
        "        A = A_transform(A)\n",
        "        B = B_transform(B)\n",
        "\n",
        "        return {\"A\": A, \"B\": B, \"A_paths\": AB_path, \"B_paths\": AB_path}\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the total number of images in the dataset.\"\"\"\n",
        "        return len(self.AB_paths)\n"
      ],
      "metadata": {
        "id": "htL2EPx3bfV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from .base_options import BaseOptions\n",
        "\n",
        "\n",
        "class TrainOptions(BaseOptions):\n",
        "    \"\"\"This class includes training options.\n",
        "\n",
        "    It also includes shared options defined in BaseOptions.\n",
        "    \"\"\"\n",
        "\n",
        "    def initialize(self, parser):\n",
        "        parser = BaseOptions.initialize(self, parser)\n",
        "        # HTML visualization parameters\n",
        "        parser.add_argument('--display_freq', type=int, default=400, help='frequency of showing training results on screen')\n",
        "        parser.add_argument('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n",
        "        parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n",
        "        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n",
        "        # network saving and loading parameters\n",
        "        parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n",
        "        parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n",
        "        parser.add_argument('--save_by_iter', action='store_true', help='whether saves model by iteration')\n",
        "        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
        "        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
        "        parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
        "        # training parameters\n",
        "        parser.add_argument('--n_epochs', type=int, default=100, help='number of epochs with the initial learning rate')\n",
        "        parser.add_argument('--n_epochs_decay', type=int, default=100, help='number of epochs to linearly decay learning rate to zero')\n",
        "        parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n",
        "        parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
        "        parser.add_argument('--gan_mode', type=str, default='lsgan', help='the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n",
        "        parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n",
        "        parser.add_argument('--lr_policy', type=str, default='linear', help='learning rate policy. [linear | step | plateau | cosine]')\n",
        "        parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n",
        "\n",
        "        self.isTrain = True\n",
        "        return parser\n"
      ],
      "metadata": {
        "id": "5YnSvloIbtFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from .base_options import BaseOptions\n",
        "\n",
        "\n",
        "class TestOptions(BaseOptions):\n",
        "    \"\"\"This class includes test options.\n",
        "\n",
        "    It also includes shared options defined in BaseOptions.\n",
        "    \"\"\"\n",
        "\n",
        "    def initialize(self, parser):\n",
        "        parser = BaseOptions.initialize(self, parser)  # define shared options\n",
        "        parser.add_argument('--results_dir', type=str, default='./results/', help='saves results here.')\n",
        "        parser.add_argument('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')\n",
        "        parser.add_argument('--phase', type=str, default='test', help='train, val, test, etc')\n",
        "        # Dropout and Batchnorm has different behavioir during training and test.\n",
        "        parser.add_argument('--eval', action='store_true', help='use eval mode during test time.')\n",
        "        parser.add_argument('--num_test', type=int, default=50, help='how many test images to run')\n",
        "        # rewrite devalue values\n",
        "        parser.set_defaults(model='test')\n",
        "        # To avoid cropping, the load_size should be the same as crop_size\n",
        "        parser.set_defaults(load_size=parser.get_default('crop_size'))\n",
        "        self.isTrain = False\n",
        "        return parser\n"
      ],
      "metadata": {
        "id": "aQpKYaXDbu4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"General-purpose training script for image-to-image translation.\n",
        "\n",
        "This script works for various models (with option '--model': e.g., pix2pix, cyclegan, colorization) and\n",
        "different datasets (with option '--dataset_mode': e.g., aligned, unaligned, single, colorization).\n",
        "You need to specify the dataset ('--dataroot'), experiment name ('--name'), and model ('--model').\n",
        "\n",
        "It first creates model, dataset, and visualizer given the option.\n",
        "It then does standard network training. During the training, it also visualize/save the images, print/save the loss plot, and save models.\n",
        "The script supports continue/resume training. Use '--continue_train' to resume your previous training.\n",
        "\n",
        "Example:\n",
        "    Train a CycleGAN model:\n",
        "        python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n",
        "    Train a pix2pix model:\n",
        "        python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n",
        "\n",
        "See options/base_options.py and options/train_options.py for more training options.\n",
        "See training and test tips at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md\n",
        "See frequently asked questions at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "from options.train_options import TrainOptions\n",
        "from data import create_dataset\n",
        "from models import create_model\n",
        "from util.visualizer import Visualizer\n",
        "from util.util import init_ddp, cleanup_ddp\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = TrainOptions().parse()  # get training options\n",
        "    opt.device = init_ddp()\n",
        "    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n",
        "    dataset_size = len(dataset)  # get the number of images in the dataset.\n",
        "    print(f\"The number of training images = {dataset_size}\")\n",
        "\n",
        "    model = create_model(opt)  # create a model given opt.model and other options\n",
        "    model.setup(opt)  # regular setup: load and print networks; create schedulers\n",
        "    visualizer = Visualizer(opt)  # create a visualizer that display/save images and plots\n",
        "    total_iters = 0  # the total number of training iterations\n",
        "    for epoch in range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1):\n",
        "        epoch_start_time = time.time()  # timer for entire epoch\n",
        "        iter_data_time = time.time()  # timer for data loading per iteration\n",
        "        epoch_iter = 0  # the number of training iterations in current epoch, reset to 0 every epoch\n",
        "        visualizer.reset()\n",
        "        # Set epoch for DistributedSampler\n",
        "        if hasattr(dataset, \"set_epoch\"):\n",
        "            dataset.set_epoch(epoch)\n",
        "\n",
        "        for i, data in enumerate(dataset):  # inner loop within one epoch\n",
        "            iter_start_time = time.time()  # timer for computation per iteration\n",
        "            if total_iters % opt.print_freq == 0:\n",
        "                t_data = iter_start_time - iter_data_time\n",
        "\n",
        "            total_iters += opt.batch_size\n",
        "            epoch_iter += opt.batch_size\n",
        "            model.set_input(data)  # unpack data from dataset and apply preprocessing\n",
        "            model.optimize_parameters()  # calculate loss functions, get gradients, update network weights\n",
        "\n",
        "            if total_iters % opt.display_freq == 0:  # display images on visdom and save images to a HTML file\n",
        "                save_result = total_iters % opt.update_html_freq == 0\n",
        "                model.compute_visuals()\n",
        "                visualizer.display_current_results(model.get_current_visuals(), epoch, total_iters, save_result)\n",
        "\n",
        "            if total_iters % opt.print_freq == 0:  # print training losses and save logging information to the disk\n",
        "                losses = model.get_current_losses()\n",
        "                t_comp = (time.time() - iter_start_time) / opt.batch_size\n",
        "                visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)\n",
        "                visualizer.plot_current_losses(total_iters, losses)\n",
        "\n",
        "            if total_iters % opt.save_latest_freq == 0:  # cache our latest model every <save_latest_freq> iterations\n",
        "                print(f\"saving the latest model (epoch {epoch}, total_iters {total_iters})\")\n",
        "                save_suffix = f\"iter_{total_iters}\" if opt.save_by_iter else \"latest\"\n",
        "                model.save_networks(save_suffix)\n",
        "\n",
        "            iter_data_time = time.time()\n",
        "\n",
        "        model.update_learning_rate()  # update learning rates at the end of every epoch\n",
        "\n",
        "        if epoch % opt.save_epoch_freq == 0:  # cache our model every <save_epoch_freq> epochs\n",
        "            print(f\"saving the model at the end of epoch {epoch}, iters {total_iters}\")\n",
        "            model.save_networks(\"latest\")\n",
        "            model.save_networks(epoch)\n",
        "\n",
        "        print(f\"End of epoch {epoch} / {opt.n_epochs + opt.n_epochs_decay} \\t Time Taken: {time.time() - epoch_start_time:.0f} sec\")\n",
        "\n",
        "    cleanup_ddp()\n"
      ],
      "metadata": {
        "id": "2YSxxyqEbzj3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}